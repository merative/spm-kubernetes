{"componentChunkName":"component---src-pages-troubleshooting-must-gather-mdx","path":"/troubleshooting/must_gather/","result":{"pageContext":{"frontmatter":{"title":"MustGather data for SPM","description":"MustGather data for SPM"},"relativePagePath":"/troubleshooting/must_gather.mdx","titleType":"page","MdxNode":{"id":"73928378-66be-5a24-97dc-f0a7dd2e0e5a","children":[],"parent":"131c8609-4be1-5abd-bcaf-bbf244ba2a3e","internal":{"content":"---\ntitle: MustGather data for SPM\ndescription: MustGather data for SPM\n---\n\n## Collecting MustGather data for SPM on Kubernetes\n\n<AnchorLinks>\n  <AnchorLink>Introduction</AnchorLink>\n  <AnchorLink>Collecting MustGather data guidelines</AnchorLink>\n  <AnchorLink>General SPM MustGather</AnchorLink>\n  <AnchorLink>Runtime systems</AnchorLink>\n  <AnchorLink>Helm information</AnchorLink>\n  <AnchorLink>Kubernetes deployment information</AnchorLink>\n  <AnchorLink>Detailed MustGather for logs and other artifacts</AnchorLink>\n</AnchorLinks>\n\n## Introduction\n\nThe information laid out in this section details the information needed when raising a support case. Note that not all information detailed is necessary when\nraising an issue on [GitHub](https://github.com/merative/spm-kubernetes/issues).\n\n<InlineNotification>\n\nMerative SPM may request the **Helm Charts** and **Dockerfiles** developed by the engineering team as part of the support process.\n\n</InlineNotification>\n\n### Question\n\nWhat information should I collect for issues with Merative Social Program Management (SPM) in a Kubernetes environment to assist Meravite Cúram Support with their investigations?\n\n### Answer\n\nFirst, a comprehensive description of the problem along with answers to the following questions:\n\n* Can you recreate the issue and provide the steps for Merative Support to do same?\n* What is the impact of the issue; for instance, frequency of occurrence and pervasiveness?\n* Have you identified any workarounds or mitigations?\n* What investigation have you already performed?\n\n## Collecting MustGather data guidelines\n\nOutlines below are must gather data guidelines that we recommend are followed.\n\n* Do not provide any unanonymized data or information.\n* Save all output to text files or files of appropriate formats. Include the command that created the output file.\n* Logs and other artifacts can only be obtained from a Kubernetes pod if the pod is running; the two exceptions are:\n  * stdout is still available via `kubectl logs` (see [stdout logs from pods](#stdout-logs-from-pods) for detail) even if the pod is in a non-running state such as completed\n  * logs that are stored persistently are available over the lifetime of that storage medium\n* Regarding command examples that follow:\n  * In commands with variable information, such as namespaces, these are represented in the example commands by angle brackets, which you must replace.\n  * Commands where additional options are possible are indicated by an ellipses: ...\n\n## General SPM MustGather\n\nBeyond the answers to these questions, you need to collect general third party software information (for example, versions) and more specific diagnostic, log, and other information for the areas where you are seeing the issue.\nThis general information is described in the sections that follow.\nFor issues with the following third party software you should consult their external MustGather links as appropriate:\n\n* [SPM](https://www.merative.com/support/spm/question-answer/collecting-data-for-issues)\n* [Liberty](https://www.ibm.com/support/pages/mustgather-read-first-websphere-application-server)\n* [Db2](https://www.ibm.com/support/pages/collecting-data-read-first-db2-linux-unix-and-windows-products)\n* [MQ](https://www.ibm.com/support/pages/read-first-collect-ibm-mq-mustgather-data-linux-unix-windows-and-ibm-i)\n\nThe following sections describe how to gather information specific to SPM running on Kubernetes and provide information relating to where logs and traces reside relating to the MustGather.\n\n### SPM and base third party software versions\n\nIn a SPM environment the Ant `configreport` target includes the following versions in its generated zip file:\n\n* SPM version -  available in the `Installerlogs/Installhistory.txt` file within the resulting `$SERVER_DIR/config_report.zip` file\n* Inside the ConfigReporter*.log file of the config folder within the resulting `$SERVER_DIR/config_report.zip` file you will find the:\n* Java version\n  * Application server version\n  * Database version\n  * Ant version\n\nTo generate the `$SERVER_DIR/config_report.zip` file run the Ant `configreport` target:\n\n```\ncd $SERVER_DIR\n./build.sh configreport\n```\n\nPlease provide the `config_report.zip` file.\n\n## Runtime systems\n\n### Operating systems and platforms\n\nInclude this information for all relevant platforms used for deployment, where you run the Azure CLI or Minikube.\n\nOn most systems the following command provides basic platform and operating system information:\n\n```\nuname -a\n```\n\n### Kubernetes and related software versions\n\nThe following commands display the relevant versions for Docker, Helm and Kubernetes:\n\n```\n  docker version # Provides version information for the Docker client and server\n  docker info    # Provides general runtime information about Docker\n\n  helm version   # Provides Helm version information\n\n  # Depending on your environment, obtain the appropriate Kubernetes version information:\n  minikube version   # If applicable\n  oc version         # If applicable (OpenShift)\n  kubectl version    # Applies to Minikube, Kubernetes and OpenShift environments\n```\n\n## Helm information\n\nFor deployments, gather information relevant or appropriate to your environment and the issue faced.\n\nFor Helm use the command appropriate to your Helm version to list the deployments in the environment, which provides the release and chart names needed in later Helm commands:\n\n```\nhelm list -a --namespace <name_space>\n```\n\n### Verify Helm Chart correctness\n\nUse the `helm lint` command to verify the correctness of your Helm Charts (prior to installation), for example:\n\n```\ncd $SPM_HOME/helm-charts/spm\nhelm lint .\n```\n\nYou should receive successful output from the `helm lint` command like this:\n\n```\n==> Linting .\nLint OK\n\n1 chart(s) linted, no failures\n```\n\nIf you encounter any errors, correct these, and recreate your issue.\n\n### Helm Chart rendering\n\nRunning the `helm template` command processes the chart and generates output similar to what would be generated during deployment;\nhowever, the `helm install` command (see the following examples) provides additional information; notably overrides provided via the install.\nSo, where possible, the output from `helm install` is preferable to that of `helm template`.\n\n```\nhelm template <release_name> <chart_name> ...\n```\n\nCheck the output to verify that the expected values appear as intended.\n\n### Helm Chart installation\n\nIf you encounter errors when installing Helm Charts use the `helm install` command with the following options to generate additional information\n\n* `--debug` - checks the generated manifests of a release without installing the chart\n* `--dry-run` - produces output similar to `helm template` but additionally executes the secrets and verifies the objects\n\nFor example:\n\n```\nhelm install <release_name> <chart_name> --debug --dry-run ...\n```\n\n### Verify Helm Chart installation\n\nThe following command provides more information about what has been deployed or is running in Kubernetes.\n\nThe `helm get all` command provides information equivalent to `helm install --debug --dry-run`. For example:\n\n```\nhelm get all <release-name> ...\n```\n\n## Kubernetes deployment information\n\n### Enquiring a Kubernetes environment\n\nIf running in a Minikube environment use the following command to confirm its status:\n\n```\nminikube status\n```\n\nUse the following commands to get information about the running Kubernetes environment.\n\nDisplay basic information about the cluster configuration:\n\n```\nkubectl config view --namespace <name_space> ...\n```\n\nDisplay basic information about the nodes and pods in the cluster:\n\n```\nkubectl get nodes -o wide --namespace <name_space> ...\nkubectl get pods -o wide --namespace <name_space> ...\n```\n\nFor each of the pods displayed by the `kubectl get pods` command use the pod name as input to the following command to get more detailed information about that pod, for example:\n\n```\nkubectl describe pod <pod_name> --namespace <name_space> ...\n```\n\nA convenient way to run the `kubectl describe pod <pod_name> --namespace <name_space> ...` command for all the pods in a namespace is as follows:\n\n```\nmkdir pod\npods=`kubectl get pods -o name --namespace <name_space>`\nfor p in $pods ; do kubectl describe $p --namespace <name_space> > $p.txt ; done\n```\n\nThe results will be in the `pod` folder created using the `kubectl` command.\n\n## MustGather to collect logs and other artifacts\n\nThe following sections identify how to obtain logs and artifacts for specific areas of Kubernetes.\n\n### Externalizing logs and artifacts inside a Kubernetes pod\n\nExternalizing logs and artifacts from a Kubernetes pod varies by the type of pod.\nThe sections that follow identify locations for this information.\nThe [kubectl command reference](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#cp) gives several examples of how to externalize files from within a pod, such as:\n\n```\nkubectl cp <name_space>/<pod_name>:/logs /tmp/logs\nkubectl exec -n <name_space> <pod_name> -- tar cf - /logs | tar xf - -C /tmp\n```\n\n**Note:** Both of the above `kubectl` command formats produce the following spurious, benign error which you can ignore:\n\n```\ntar: Removing leading `/' from member names\n```\n\n### Minikube logs\n\nUse the `minikube logs` command to obtain logs for the Minikube environment.\n\nYou can set the logging level when starting Minikube via the `--v` option of the `minikube start` command.\nFor instance, and to conveniently save the output at the same time:\n\n```\nminikube start --alsologtostderr --v=3 2>&1 | tee /tmp/minikube-start.log\n```\n\nNoting that the log output at startup differs from the output from the runtime log content via `minikube logs`.\n\nSee [Minikube documentation](https://minikube.sigs.k8s.io/docs/) for more information.\n\n### stdout logs from pods\n\nThe `kubectl logs` provides the stdout logs for a specified Kubernetes pod; so the log data provided is specific to the type of pod or software running within it.\nFor instance, if a pod is running WebSphere Liberty this command provides the equivalent of the contents of the WebSphere Liberty `console.log` file\nand for other pods will provide stdout log output specific to that pod's context; for instance MQ, Cúram XML Server, or Cúram batch.\n\nThe command has various options, such as `--follow`, `--tail`, and `--since` to control and display the log output.\nSee the `kubectl help logs` command for option details.\n\n### Third party software logs and artifacts\n\nBeyond basic stdout logging from a pod there may be additional logging or artifacts needed for investigating an issue.\nThese logs and artifacts will be available within the pod's file system or, in the case of object storage, in the persistent storage location specified\n[Cloud Object Storage](/supporting-infrastructure/COS).\n\nFurther, the specific logs, artifacts, and their locations vary depending on the software running within the pod.\n\nThe following sections identify these various locations.\n\n#### WebSphere Liberty stdout logs\n\nWebSphere Liberty pods are identified by the pod naming pattern beginning with: `<helm_release_name>-apps-` or `<helm_release_name>-uawebapp-`.\n\nThus, if your Helm release was named \"purpleclown\" you would find pod names like these from a `kubectl get pods` command:\n\n* `purpleclown-apps-curam-consumer-7c578b9b59-t487n`\n* `purpleclown-apps-curam-producer-6794fc9b94-4srxr`\n* `purpleclown-apps-rest-producer-6858d55bd-x6z6s`\n* `purpleclown-apps-rest-consumer-5567ddc4cd-qpwp7`\n* `purpleclown-uawebapp-54b56d5866-2mnk5`\n\nwhere the number of pods may vary depending on the applications deployed and the number of replicas specified.\n\nThe default log location for WebSphere Liberty is the `/logs` folder of the pod's file system and, for example, can be accessed with the following command\n\n```\nkubectl exec --namespace <name_space> -it <pod_name> -- ls -l /logs\n```\n\nwhich would produce output like:\n\n```\n-rw-r----- 1 default root 62334 Apr 14 06:01 messages.log\n-rw-r--r-- 1 default root  2134 Mar 19 06:44 messages_20.04.14_06.00.47.0.log\n-rw-r----- 1 default root   973 Apr 14 06:00 trace.log\n```\n\nWhen using object storage the WebSphere Liberty logs, JVM dumps and garbage collection logs, and Cúram JMX statistics are symbolically linked to the pod's file system in `/tmp/logs` and available in your COS bucket.\n\n#### WebSphere Liberty configuration information\n\nWebSphere Liberty configuration information can be found in the pod's file system in `/liberty/wlp/usr/servers/defaultServer`, which contains:\n\n```\nserver.xml\njvm.options\nserver.env\nbootstrap.properties\nadc_conf/\n```\n\nThe WebSphere Liberty `server package` command can be used to zip up the server; but, the logs folder, being stored separately, must be handled directly from the `/logs` folder.\n\n#### MQ logs and artifacts\n\nMQ pods are identified by the pod naming pattern beginning with: `<helm_release_name>-mqserverapps-`.\n\nSo, if your Helm release was named \"purpleclown\" you would find pod names like the following from a `kubectl get pods` command:\n\n* `purpleclown-mqserver-curam-557d64c76-7sm8r`\n* `purpleclown-mqserver-rest-dbb994dc8-4p5cv`\n\nwhere the number of pods may vary depending on the SPM applications deployed.\n\nThe location of MQ logs is inside the MQ pods is as documented in the [MQ IBM Documentation](https://www.ibm.com/docs/en/ibm-mq/9.1?topic=windows-error-log-directories-unix-linux).\nYou can interrogate the various log, error, and trace locations identified in the MQ IBM Documentation (and its MustGather) by accessing the MQ pod's shell; for example:\n\n```\nkubectl exec --namespace <name_space> -it <mq_pod_name> -- /bin/bash\n```\n\n#### Cúram XML Server logs and artifacts\n\nCúram XML Server pods are identified by the pod naming pattern beginning with: `<helm_release_name>-xmlserver-`.\n\nSo, if your Helm release was named \"purpleclown\" you would find pods named like the following from a `kubectl get pods` command:\n\n* `purpleclown-xmlserver-86b588c784-z76mz`\n\nwhere the number of pods may vary depending on the number of replicas specified.\n\nUsing the `kubectl exec` command as illustrated will place you into the default XML Server directory `/opt/ibm/Curam/xmlserver`:\n\n```\nkubectl exec --namespace <name_space> -it <xmlserver_pod_name> -- /bin/bash\n```\n\nThe log file in `/opt/ibm/Curam/xmlserver/tmp/xmlserver.log` is the same as the stdout produced by the `kubectl logs` command.\n\nFor more information about XML server tracing and logging, see *Configuring the XML server** in the *Social Program Management XML Infrastructure Guide*. Gathering this data is beyond the scope of this document.\n\n<InlineNotification>\n\nThe Social Program Management PDF documentation is available to download from [Merative Support Docs](https://curam-spm-devops.github.io/wh-support-docs/spm/pdf-documentation/).\n\n</InlineNotification>\n\n#### Cúram batch logs and artifacts\n\nCúram batch pods are identified by the pod naming pattern beginning with: `<helm_release_name>-batch-`.\n\nSo, if your Helm release was named \"purpleclown\" you would find pods named like the following from a kubectl get pods command:\n\n* `purpleclown-batch-1587036600-9rrnw`\n\nwhere the number of pods may vary depending on the number of replicas specified.\n\nUse the `kubctl logs` command to view the batch pod's stdout.\n\nFor these pods the relevant log files can be found within the `/opt/ibm/Curam/release/buildlogs` folder of the pod’s file system\n\nUsing the `kubectl exec` command as illustrated will place you into the pod:\n\n```\nkubectl exec --namespace <name_space> -it <batch_pod_name> -- /bin/bash\n```\n\nThe log file is in `/opt/ibm/Curam/release/buildlogs/BatchLaucher123456789.log`.\n\nIf the persistence volume is enabled to store data, it will be saved to the Azure blob storage account,\n\nin the \"logs\" directory within the pod's directory.\n\nIf persistence is not enabled, the logs will be generated in the folder indicated above by default.\n\nEnabling persistence activates the parameter\n\n```\n-Ddir.bld.log=$MOUNT_POINT/$HOSTNAME/logs\n```\n\nIn Batch, the -Ddir.bld.log option is used to specify a system property that defines the path to the directory where\n\nthe build logs will be stored. This option is typically used in the context of build commands or automation scripts to\n\ndynamically configure properties without modifying configuration files.\n\nConfiguring logs correctly is crucial for debugging and maintaining the build process.\n\nThe -Ddir.bld.log option is a flexible and powerful way to dynamically configure the log directory for Batch build processes.\n\n$MOUNT_POINT/$HOSTNAME/logs is a folder created adhoc to store the logs in the Azure blob storage account.\n\n#### HTTP Server logs and artifacts\n\nThe Apache HTTP Server is only used for hosting SPM static content this information would only be of interest for issues relating specifically to static content.\n\nHTTP Server pods are identified by the pod naming pattern beginning with: `<helm_release_name>-web-`.\n\nSo, if your Helm release was named \"purpleclown\" you would find pods named like the following from a `kubectl get pods` command:\n\n* `purpleclown-web-5c9cf5d868-ncdqk`\n\nwhere the number of pods may vary depending on the number of replicas specified.\n\nFor these pods the relevant log files can be found within the `/var/log/httpd` folder of the pod's file system.\nFor instance:\n\n```\nkubectl exec --namespace <name_space> -it <pod_name> -- ls -l /var/log/httpd/error_log /var/log/httpd/access_log\n```\n\nwould show the relevant logs:\n\n```\n-rw-r--r-- 1 default root    7826 Jul 28 09:32 /var/log/httpd/access_log\n-rw-r--r-- 1 default root    3297 Jul 28 09:25 /var/log/httpd/error_log\n```\n","type":"Mdx","contentDigest":"246502b6c6e021cb16b563ffff878365","owner":"gatsby-plugin-mdx","counter":194},"frontmatter":{"title":"MustGather data for SPM","description":"MustGather data for SPM"},"exports":{},"rawBody":"---\ntitle: MustGather data for SPM\ndescription: MustGather data for SPM\n---\n\n## Collecting MustGather data for SPM on Kubernetes\n\n<AnchorLinks>\n  <AnchorLink>Introduction</AnchorLink>\n  <AnchorLink>Collecting MustGather data guidelines</AnchorLink>\n  <AnchorLink>General SPM MustGather</AnchorLink>\n  <AnchorLink>Runtime systems</AnchorLink>\n  <AnchorLink>Helm information</AnchorLink>\n  <AnchorLink>Kubernetes deployment information</AnchorLink>\n  <AnchorLink>Detailed MustGather for logs and other artifacts</AnchorLink>\n</AnchorLinks>\n\n## Introduction\n\nThe information laid out in this section details the information needed when raising a support case. Note that not all information detailed is necessary when\nraising an issue on [GitHub](https://github.com/merative/spm-kubernetes/issues).\n\n<InlineNotification>\n\nMerative SPM may request the **Helm Charts** and **Dockerfiles** developed by the engineering team as part of the support process.\n\n</InlineNotification>\n\n### Question\n\nWhat information should I collect for issues with Merative Social Program Management (SPM) in a Kubernetes environment to assist Meravite Cúram Support with their investigations?\n\n### Answer\n\nFirst, a comprehensive description of the problem along with answers to the following questions:\n\n* Can you recreate the issue and provide the steps for Merative Support to do same?\n* What is the impact of the issue; for instance, frequency of occurrence and pervasiveness?\n* Have you identified any workarounds or mitigations?\n* What investigation have you already performed?\n\n## Collecting MustGather data guidelines\n\nOutlines below are must gather data guidelines that we recommend are followed.\n\n* Do not provide any unanonymized data or information.\n* Save all output to text files or files of appropriate formats. Include the command that created the output file.\n* Logs and other artifacts can only be obtained from a Kubernetes pod if the pod is running; the two exceptions are:\n  * stdout is still available via `kubectl logs` (see [stdout logs from pods](#stdout-logs-from-pods) for detail) even if the pod is in a non-running state such as completed\n  * logs that are stored persistently are available over the lifetime of that storage medium\n* Regarding command examples that follow:\n  * In commands with variable information, such as namespaces, these are represented in the example commands by angle brackets, which you must replace.\n  * Commands where additional options are possible are indicated by an ellipses: ...\n\n## General SPM MustGather\n\nBeyond the answers to these questions, you need to collect general third party software information (for example, versions) and more specific diagnostic, log, and other information for the areas where you are seeing the issue.\nThis general information is described in the sections that follow.\nFor issues with the following third party software you should consult their external MustGather links as appropriate:\n\n* [SPM](https://www.merative.com/support/spm/question-answer/collecting-data-for-issues)\n* [Liberty](https://www.ibm.com/support/pages/mustgather-read-first-websphere-application-server)\n* [Db2](https://www.ibm.com/support/pages/collecting-data-read-first-db2-linux-unix-and-windows-products)\n* [MQ](https://www.ibm.com/support/pages/read-first-collect-ibm-mq-mustgather-data-linux-unix-windows-and-ibm-i)\n\nThe following sections describe how to gather information specific to SPM running on Kubernetes and provide information relating to where logs and traces reside relating to the MustGather.\n\n### SPM and base third party software versions\n\nIn a SPM environment the Ant `configreport` target includes the following versions in its generated zip file:\n\n* SPM version -  available in the `Installerlogs/Installhistory.txt` file within the resulting `$SERVER_DIR/config_report.zip` file\n* Inside the ConfigReporter*.log file of the config folder within the resulting `$SERVER_DIR/config_report.zip` file you will find the:\n* Java version\n  * Application server version\n  * Database version\n  * Ant version\n\nTo generate the `$SERVER_DIR/config_report.zip` file run the Ant `configreport` target:\n\n```\ncd $SERVER_DIR\n./build.sh configreport\n```\n\nPlease provide the `config_report.zip` file.\n\n## Runtime systems\n\n### Operating systems and platforms\n\nInclude this information for all relevant platforms used for deployment, where you run the Azure CLI or Minikube.\n\nOn most systems the following command provides basic platform and operating system information:\n\n```\nuname -a\n```\n\n### Kubernetes and related software versions\n\nThe following commands display the relevant versions for Docker, Helm and Kubernetes:\n\n```\n  docker version # Provides version information for the Docker client and server\n  docker info    # Provides general runtime information about Docker\n\n  helm version   # Provides Helm version information\n\n  # Depending on your environment, obtain the appropriate Kubernetes version information:\n  minikube version   # If applicable\n  oc version         # If applicable (OpenShift)\n  kubectl version    # Applies to Minikube, Kubernetes and OpenShift environments\n```\n\n## Helm information\n\nFor deployments, gather information relevant or appropriate to your environment and the issue faced.\n\nFor Helm use the command appropriate to your Helm version to list the deployments in the environment, which provides the release and chart names needed in later Helm commands:\n\n```\nhelm list -a --namespace <name_space>\n```\n\n### Verify Helm Chart correctness\n\nUse the `helm lint` command to verify the correctness of your Helm Charts (prior to installation), for example:\n\n```\ncd $SPM_HOME/helm-charts/spm\nhelm lint .\n```\n\nYou should receive successful output from the `helm lint` command like this:\n\n```\n==> Linting .\nLint OK\n\n1 chart(s) linted, no failures\n```\n\nIf you encounter any errors, correct these, and recreate your issue.\n\n### Helm Chart rendering\n\nRunning the `helm template` command processes the chart and generates output similar to what would be generated during deployment;\nhowever, the `helm install` command (see the following examples) provides additional information; notably overrides provided via the install.\nSo, where possible, the output from `helm install` is preferable to that of `helm template`.\n\n```\nhelm template <release_name> <chart_name> ...\n```\n\nCheck the output to verify that the expected values appear as intended.\n\n### Helm Chart installation\n\nIf you encounter errors when installing Helm Charts use the `helm install` command with the following options to generate additional information\n\n* `--debug` - checks the generated manifests of a release without installing the chart\n* `--dry-run` - produces output similar to `helm template` but additionally executes the secrets and verifies the objects\n\nFor example:\n\n```\nhelm install <release_name> <chart_name> --debug --dry-run ...\n```\n\n### Verify Helm Chart installation\n\nThe following command provides more information about what has been deployed or is running in Kubernetes.\n\nThe `helm get all` command provides information equivalent to `helm install --debug --dry-run`. For example:\n\n```\nhelm get all <release-name> ...\n```\n\n## Kubernetes deployment information\n\n### Enquiring a Kubernetes environment\n\nIf running in a Minikube environment use the following command to confirm its status:\n\n```\nminikube status\n```\n\nUse the following commands to get information about the running Kubernetes environment.\n\nDisplay basic information about the cluster configuration:\n\n```\nkubectl config view --namespace <name_space> ...\n```\n\nDisplay basic information about the nodes and pods in the cluster:\n\n```\nkubectl get nodes -o wide --namespace <name_space> ...\nkubectl get pods -o wide --namespace <name_space> ...\n```\n\nFor each of the pods displayed by the `kubectl get pods` command use the pod name as input to the following command to get more detailed information about that pod, for example:\n\n```\nkubectl describe pod <pod_name> --namespace <name_space> ...\n```\n\nA convenient way to run the `kubectl describe pod <pod_name> --namespace <name_space> ...` command for all the pods in a namespace is as follows:\n\n```\nmkdir pod\npods=`kubectl get pods -o name --namespace <name_space>`\nfor p in $pods ; do kubectl describe $p --namespace <name_space> > $p.txt ; done\n```\n\nThe results will be in the `pod` folder created using the `kubectl` command.\n\n## MustGather to collect logs and other artifacts\n\nThe following sections identify how to obtain logs and artifacts for specific areas of Kubernetes.\n\n### Externalizing logs and artifacts inside a Kubernetes pod\n\nExternalizing logs and artifacts from a Kubernetes pod varies by the type of pod.\nThe sections that follow identify locations for this information.\nThe [kubectl command reference](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#cp) gives several examples of how to externalize files from within a pod, such as:\n\n```\nkubectl cp <name_space>/<pod_name>:/logs /tmp/logs\nkubectl exec -n <name_space> <pod_name> -- tar cf - /logs | tar xf - -C /tmp\n```\n\n**Note:** Both of the above `kubectl` command formats produce the following spurious, benign error which you can ignore:\n\n```\ntar: Removing leading `/' from member names\n```\n\n### Minikube logs\n\nUse the `minikube logs` command to obtain logs for the Minikube environment.\n\nYou can set the logging level when starting Minikube via the `--v` option of the `minikube start` command.\nFor instance, and to conveniently save the output at the same time:\n\n```\nminikube start --alsologtostderr --v=3 2>&1 | tee /tmp/minikube-start.log\n```\n\nNoting that the log output at startup differs from the output from the runtime log content via `minikube logs`.\n\nSee [Minikube documentation](https://minikube.sigs.k8s.io/docs/) for more information.\n\n### stdout logs from pods\n\nThe `kubectl logs` provides the stdout logs for a specified Kubernetes pod; so the log data provided is specific to the type of pod or software running within it.\nFor instance, if a pod is running WebSphere Liberty this command provides the equivalent of the contents of the WebSphere Liberty `console.log` file\nand for other pods will provide stdout log output specific to that pod's context; for instance MQ, Cúram XML Server, or Cúram batch.\n\nThe command has various options, such as `--follow`, `--tail`, and `--since` to control and display the log output.\nSee the `kubectl help logs` command for option details.\n\n### Third party software logs and artifacts\n\nBeyond basic stdout logging from a pod there may be additional logging or artifacts needed for investigating an issue.\nThese logs and artifacts will be available within the pod's file system or, in the case of object storage, in the persistent storage location specified\n[Cloud Object Storage](/supporting-infrastructure/COS).\n\nFurther, the specific logs, artifacts, and their locations vary depending on the software running within the pod.\n\nThe following sections identify these various locations.\n\n#### WebSphere Liberty stdout logs\n\nWebSphere Liberty pods are identified by the pod naming pattern beginning with: `<helm_release_name>-apps-` or `<helm_release_name>-uawebapp-`.\n\nThus, if your Helm release was named \"purpleclown\" you would find pod names like these from a `kubectl get pods` command:\n\n* `purpleclown-apps-curam-consumer-7c578b9b59-t487n`\n* `purpleclown-apps-curam-producer-6794fc9b94-4srxr`\n* `purpleclown-apps-rest-producer-6858d55bd-x6z6s`\n* `purpleclown-apps-rest-consumer-5567ddc4cd-qpwp7`\n* `purpleclown-uawebapp-54b56d5866-2mnk5`\n\nwhere the number of pods may vary depending on the applications deployed and the number of replicas specified.\n\nThe default log location for WebSphere Liberty is the `/logs` folder of the pod's file system and, for example, can be accessed with the following command\n\n```\nkubectl exec --namespace <name_space> -it <pod_name> -- ls -l /logs\n```\n\nwhich would produce output like:\n\n```\n-rw-r----- 1 default root 62334 Apr 14 06:01 messages.log\n-rw-r--r-- 1 default root  2134 Mar 19 06:44 messages_20.04.14_06.00.47.0.log\n-rw-r----- 1 default root   973 Apr 14 06:00 trace.log\n```\n\nWhen using object storage the WebSphere Liberty logs, JVM dumps and garbage collection logs, and Cúram JMX statistics are symbolically linked to the pod's file system in `/tmp/logs` and available in your COS bucket.\n\n#### WebSphere Liberty configuration information\n\nWebSphere Liberty configuration information can be found in the pod's file system in `/liberty/wlp/usr/servers/defaultServer`, which contains:\n\n```\nserver.xml\njvm.options\nserver.env\nbootstrap.properties\nadc_conf/\n```\n\nThe WebSphere Liberty `server package` command can be used to zip up the server; but, the logs folder, being stored separately, must be handled directly from the `/logs` folder.\n\n#### MQ logs and artifacts\n\nMQ pods are identified by the pod naming pattern beginning with: `<helm_release_name>-mqserverapps-`.\n\nSo, if your Helm release was named \"purpleclown\" you would find pod names like the following from a `kubectl get pods` command:\n\n* `purpleclown-mqserver-curam-557d64c76-7sm8r`\n* `purpleclown-mqserver-rest-dbb994dc8-4p5cv`\n\nwhere the number of pods may vary depending on the SPM applications deployed.\n\nThe location of MQ logs is inside the MQ pods is as documented in the [MQ IBM Documentation](https://www.ibm.com/docs/en/ibm-mq/9.1?topic=windows-error-log-directories-unix-linux).\nYou can interrogate the various log, error, and trace locations identified in the MQ IBM Documentation (and its MustGather) by accessing the MQ pod's shell; for example:\n\n```\nkubectl exec --namespace <name_space> -it <mq_pod_name> -- /bin/bash\n```\n\n#### Cúram XML Server logs and artifacts\n\nCúram XML Server pods are identified by the pod naming pattern beginning with: `<helm_release_name>-xmlserver-`.\n\nSo, if your Helm release was named \"purpleclown\" you would find pods named like the following from a `kubectl get pods` command:\n\n* `purpleclown-xmlserver-86b588c784-z76mz`\n\nwhere the number of pods may vary depending on the number of replicas specified.\n\nUsing the `kubectl exec` command as illustrated will place you into the default XML Server directory `/opt/ibm/Curam/xmlserver`:\n\n```\nkubectl exec --namespace <name_space> -it <xmlserver_pod_name> -- /bin/bash\n```\n\nThe log file in `/opt/ibm/Curam/xmlserver/tmp/xmlserver.log` is the same as the stdout produced by the `kubectl logs` command.\n\nFor more information about XML server tracing and logging, see *Configuring the XML server** in the *Social Program Management XML Infrastructure Guide*. Gathering this data is beyond the scope of this document.\n\n<InlineNotification>\n\nThe Social Program Management PDF documentation is available to download from [Merative Support Docs](https://curam-spm-devops.github.io/wh-support-docs/spm/pdf-documentation/).\n\n</InlineNotification>\n\n#### Cúram batch logs and artifacts\n\nCúram batch pods are identified by the pod naming pattern beginning with: `<helm_release_name>-batch-`.\n\nSo, if your Helm release was named \"purpleclown\" you would find pods named like the following from a kubectl get pods command:\n\n* `purpleclown-batch-1587036600-9rrnw`\n\nwhere the number of pods may vary depending on the number of replicas specified.\n\nUse the `kubctl logs` command to view the batch pod's stdout.\n\nFor these pods the relevant log files can be found within the `/opt/ibm/Curam/release/buildlogs` folder of the pod’s file system\n\nUsing the `kubectl exec` command as illustrated will place you into the pod:\n\n```\nkubectl exec --namespace <name_space> -it <batch_pod_name> -- /bin/bash\n```\n\nThe log file is in `/opt/ibm/Curam/release/buildlogs/BatchLaucher123456789.log`.\n\nIf the persistence volume is enabled to store data, it will be saved to the Azure blob storage account,\n\nin the \"logs\" directory within the pod's directory.\n\nIf persistence is not enabled, the logs will be generated in the folder indicated above by default.\n\nEnabling persistence activates the parameter\n\n```\n-Ddir.bld.log=$MOUNT_POINT/$HOSTNAME/logs\n```\n\nIn Batch, the -Ddir.bld.log option is used to specify a system property that defines the path to the directory where\n\nthe build logs will be stored. This option is typically used in the context of build commands or automation scripts to\n\ndynamically configure properties without modifying configuration files.\n\nConfiguring logs correctly is crucial for debugging and maintaining the build process.\n\nThe -Ddir.bld.log option is a flexible and powerful way to dynamically configure the log directory for Batch build processes.\n\n$MOUNT_POINT/$HOSTNAME/logs is a folder created adhoc to store the logs in the Azure blob storage account.\n\n#### HTTP Server logs and artifacts\n\nThe Apache HTTP Server is only used for hosting SPM static content this information would only be of interest for issues relating specifically to static content.\n\nHTTP Server pods are identified by the pod naming pattern beginning with: `<helm_release_name>-web-`.\n\nSo, if your Helm release was named \"purpleclown\" you would find pods named like the following from a `kubectl get pods` command:\n\n* `purpleclown-web-5c9cf5d868-ncdqk`\n\nwhere the number of pods may vary depending on the number of replicas specified.\n\nFor these pods the relevant log files can be found within the `/var/log/httpd` folder of the pod's file system.\nFor instance:\n\n```\nkubectl exec --namespace <name_space> -it <pod_name> -- ls -l /var/log/httpd/error_log /var/log/httpd/access_log\n```\n\nwould show the relevant logs:\n\n```\n-rw-r--r-- 1 default root    7826 Jul 28 09:32 /var/log/httpd/access_log\n-rw-r--r-- 1 default root    3297 Jul 28 09:25 /var/log/httpd/error_log\n```\n","fileAbsolutePath":"/home/runner/work/spm-kubernetes/spm-kubernetes/src/pages/troubleshooting/must_gather.mdx"}}},"staticQueryHashes":["1364590287","137577622","2102389209","2456312558","2746626797","3018647132","3037994772","56986546","768070550"]}