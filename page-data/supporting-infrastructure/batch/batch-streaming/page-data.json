{"componentChunkName":"component---src-pages-supporting-infrastructure-batch-batch-streaming-mdx","path":"/supporting-infrastructure/batch/batch-streaming/","result":{"pageContext":{"frontmatter":{"title":"Batch Streaming","description":"Batch Streaming in Kubernetes","tabs":["Batch Processing","Batch Streaming"]},"relativePagePath":"/supporting-infrastructure/batch/batch-streaming.mdx","titleType":"page","MdxNode":{"id":"84c89d32-8bcb-5c1f-a48b-2b9a371e6376","children":[],"parent":"fb5f58ed-5356-5b13-b24c-7db48d5030a3","internal":{"content":"---\ntitle: Batch Streaming\ndescription: Batch Streaming in Kubernetes\ntabs: ['Batch Processing', 'Batch Streaming']\n---\n\n## How batch streaming is deployed in Kubernetes\n\nMerative Social Program Management (SPM) on Kubernetes supports a different model for batch processing in AKS, where, as outlined earlier, SPM batch processing can be built and deployed into its own pod.\nBy running SPM batch processing in its own pod, the pod can leverage the benefits of flexibility, elasticity, efficiency and the strategic value offered by cloud native architecture.\n\n### What is batch streaming?\n\nThe batch streaming infrastructure provides a straightforward mechanism to implement a batch process so that it can be run in parallel (streams) across multiple pods.\nFor example, if we wanted to issue payments, the chunker identifies all the cases to be paid and the stream would process a case and issue the payments that are due.\n\n![spm batch on kubernetes](../../../images/spm_batch_processing_on_kubernetes.png)\n<Caption>\n\n*Figure 1:* SPM batch processing on kubernetes\n\n</Caption>\n\nFor more information about batch streaming architecture, see **Batch Streaming Architecture** in the *Social Program Management Cúram Batch Performance Mechanisms*.\n\n<InlineNotification>\n\nThe Social Program Management PDF documentation is available to download from [Merative Support Docs](https://curam-spm-devops.github.io/wh-support-docs/spm/pdf-documentation/).\n\n</InlineNotification>\n\n## Setting up Batch Streaming with Helm\n\nStreamed jobs may be scheduled using Helm the same way as standalone Batch jobs.\n\nIn your override values file, add a section under the `batch.streamed` key for each of the streamed jobs to be scheduled.\n\n```yaml\nbatch:\n  streamed:\n    cash_reassessment:\n      schedule: \"0 1 * * *\"\n      chunker:\n        className: curam.core.sl.infrastructure.assessment.intf.CREOLEBulkCaseChunkReassessmentByProduct.process\n        parameters: productID=4100\n        replicaCount: 1\n      stream:\n        className: curam.core.sl.infrastructure.assessment.intf.CREOLEBulkCaseChunkReassessmentStream.process\n        replicaCount: 1\n    food_reassessment:\n      schedule: \"0 2 * * */2\"\n      chunker:\n        className: curam.core.sl.infrastructure.assessment.intf.CREOLEBulkCaseChunkReassessmentByProduct.process\n        parameters: productID=4200\n        replicaCount: 1\n      stream:\n        className: curam.core.sl.infrastructure.assessment.intf.CREOLEBulkCaseChunkReassessmentStream.process\n        replicaCount: 1\n```\n\nThis example schedules Cash Assistance bulk reassessment to run at 1 AM every night, and Food Assistance bulk reassessment to run at 2 AM every second night.\n\nThe `cash_reassessment` and `food_reassessment` keys have no special meaning - their purpose is to provide separate definitions of different batch jobs.\nThese keys are used in names of the CronJob objects, e.g. `<releaseName>-batch-food-reassessment-chunker` and `<releaseName>-batch-food-reassessment-stream`\n\nA complete list of configurable options for the chunkers and streams is available on the Helm [Configuration Reference](/deployment/config-reference#batch-jobs) page.\n\n## Ad-hoc execution of Batch Streaming jobs\n\nAn example for the steps required to set up Batch Streaming is outlined as follows.\nThis example uses the bulk reassessment of food assistance case types.\n\nThe first stage is to set up a new YAML file for the streaming and chunking batch processing.\n\n<InlineNotification>\n\n**Note:** No SPM default installation settings were changed.\n\n</InlineNotification>\n\n```shell\nkubectl create job --from=cronjob/${release_name}-batch-queued \\\n       -n $namespace -o yaml --dry-run=client process_name > process_name.yaml\n```\n\nWhere:\n\n* `namespace` is the namespace where you want to run the batch processing\n* `release_name` is the name of the release you are using\n* `process_name` is the name of the batch process you are creating\n\nYou should also create a new file for each process, for example `stream_foodassistance.yaml`, and `chunker_foodassistance.yaml`\n\nA corresponding YAML file is created. Open it with your preferred editor and add the following lines to the `spec.template.spec.containers[0]` section:\n\n<Tabs>\n\n<Tab label=\"Chunker\">\n<Row>\n<Column>\n\n```yaml\nargs:\n- -Dbatch.program=curam.core.sl.infrastructure.assessment.intf.CREOLEBulkCaseChunkReassessmentByProduct.process\n- -Dbatch.parameters=\"productID=4200\"\n```\n\n**Note:** The `args` key should become a sibling of the `command` key.\n\n</Column>\n</Row>\n</Tab>\n\n<Tab label=\"Stream\">\n<Row>\n<Column>\n\n```yaml\nargs:\n- -Dbatch.program=curam.core.sl.infrastructure.assessment.intf.CREOLEBulkCaseChunkReassessmentStream.process\n```\n\n**Note:** The `args` key should become a sibling of the `command` key.\n\n</Column>\n</Row>\n</Tab>\n\n</Tabs>\n\nYou should now have YAML files for a chunker and streamer for bulk reassessment of food assistance case types.\n\n<InlineNotification>\n\nNewer versions of `kubectl` add a `metadata.ownerReferences` field to the output of `kubectl create job --from=...`.\nThis field must be removed before creating the new jobs from these YAML files.\n\n</InlineNotification>\n\n### Running batch streaming yaml files\n\nTo orchestrate the batch process, run the following command and repeat for the chunker and streamer.\n\n```shell\nkubectl create -f stream_foodassistance.yaml -n $namespace\nkubectl create -f chunker_foodassistance.yaml -n $namespace\n```\n\n### Post batch processing\n\nThe batch pods that are created for batch streaming are on demand.\n\nWhen the batch processes finish, the pods remain in a `completed` state.\nWhile this does not consume CPU or Memory resources on the worker nodes of your cluster, this may place added pressure on the Kubernetes API server\n\n<InlineNotification>\n\nA [TTL Controller for Finished Resources](https://kubernetes.io/docs/concepts/workloads/controllers/ttlafterfinished/) is available in Kubernetes, which will automatically clean up any completed pods after a specified length of time.\nHowever, this feature is in Alpha state and has to be enabled using a feature flag on both the `kube-apiserver` and `kube-controller-manager`.\n\n**Note:** It is not recommended to use Alpha features in a production environment.\n\n</InlineNotification>\n","type":"Mdx","contentDigest":"e815464b9e0dce09b960c9ae39ae1027","owner":"gatsby-plugin-mdx","counter":223},"frontmatter":{"title":"Batch Streaming","description":"Batch Streaming in Kubernetes","tabs":["Batch Processing","Batch Streaming"]},"exports":{},"rawBody":"---\ntitle: Batch Streaming\ndescription: Batch Streaming in Kubernetes\ntabs: ['Batch Processing', 'Batch Streaming']\n---\n\n## How batch streaming is deployed in Kubernetes\n\nMerative Social Program Management (SPM) on Kubernetes supports a different model for batch processing in AKS, where, as outlined earlier, SPM batch processing can be built and deployed into its own pod.\nBy running SPM batch processing in its own pod, the pod can leverage the benefits of flexibility, elasticity, efficiency and the strategic value offered by cloud native architecture.\n\n### What is batch streaming?\n\nThe batch streaming infrastructure provides a straightforward mechanism to implement a batch process so that it can be run in parallel (streams) across multiple pods.\nFor example, if we wanted to issue payments, the chunker identifies all the cases to be paid and the stream would process a case and issue the payments that are due.\n\n![spm batch on kubernetes](../../../images/spm_batch_processing_on_kubernetes.png)\n<Caption>\n\n*Figure 1:* SPM batch processing on kubernetes\n\n</Caption>\n\nFor more information about batch streaming architecture, see **Batch Streaming Architecture** in the *Social Program Management Cúram Batch Performance Mechanisms*.\n\n<InlineNotification>\n\nThe Social Program Management PDF documentation is available to download from [Merative Support Docs](https://curam-spm-devops.github.io/wh-support-docs/spm/pdf-documentation/).\n\n</InlineNotification>\n\n## Setting up Batch Streaming with Helm\n\nStreamed jobs may be scheduled using Helm the same way as standalone Batch jobs.\n\nIn your override values file, add a section under the `batch.streamed` key for each of the streamed jobs to be scheduled.\n\n```yaml\nbatch:\n  streamed:\n    cash_reassessment:\n      schedule: \"0 1 * * *\"\n      chunker:\n        className: curam.core.sl.infrastructure.assessment.intf.CREOLEBulkCaseChunkReassessmentByProduct.process\n        parameters: productID=4100\n        replicaCount: 1\n      stream:\n        className: curam.core.sl.infrastructure.assessment.intf.CREOLEBulkCaseChunkReassessmentStream.process\n        replicaCount: 1\n    food_reassessment:\n      schedule: \"0 2 * * */2\"\n      chunker:\n        className: curam.core.sl.infrastructure.assessment.intf.CREOLEBulkCaseChunkReassessmentByProduct.process\n        parameters: productID=4200\n        replicaCount: 1\n      stream:\n        className: curam.core.sl.infrastructure.assessment.intf.CREOLEBulkCaseChunkReassessmentStream.process\n        replicaCount: 1\n```\n\nThis example schedules Cash Assistance bulk reassessment to run at 1 AM every night, and Food Assistance bulk reassessment to run at 2 AM every second night.\n\nThe `cash_reassessment` and `food_reassessment` keys have no special meaning - their purpose is to provide separate definitions of different batch jobs.\nThese keys are used in names of the CronJob objects, e.g. `<releaseName>-batch-food-reassessment-chunker` and `<releaseName>-batch-food-reassessment-stream`\n\nA complete list of configurable options for the chunkers and streams is available on the Helm [Configuration Reference](/deployment/config-reference#batch-jobs) page.\n\n## Ad-hoc execution of Batch Streaming jobs\n\nAn example for the steps required to set up Batch Streaming is outlined as follows.\nThis example uses the bulk reassessment of food assistance case types.\n\nThe first stage is to set up a new YAML file for the streaming and chunking batch processing.\n\n<InlineNotification>\n\n**Note:** No SPM default installation settings were changed.\n\n</InlineNotification>\n\n```shell\nkubectl create job --from=cronjob/${release_name}-batch-queued \\\n       -n $namespace -o yaml --dry-run=client process_name > process_name.yaml\n```\n\nWhere:\n\n* `namespace` is the namespace where you want to run the batch processing\n* `release_name` is the name of the release you are using\n* `process_name` is the name of the batch process you are creating\n\nYou should also create a new file for each process, for example `stream_foodassistance.yaml`, and `chunker_foodassistance.yaml`\n\nA corresponding YAML file is created. Open it with your preferred editor and add the following lines to the `spec.template.spec.containers[0]` section:\n\n<Tabs>\n\n<Tab label=\"Chunker\">\n<Row>\n<Column>\n\n```yaml\nargs:\n- -Dbatch.program=curam.core.sl.infrastructure.assessment.intf.CREOLEBulkCaseChunkReassessmentByProduct.process\n- -Dbatch.parameters=\"productID=4200\"\n```\n\n**Note:** The `args` key should become a sibling of the `command` key.\n\n</Column>\n</Row>\n</Tab>\n\n<Tab label=\"Stream\">\n<Row>\n<Column>\n\n```yaml\nargs:\n- -Dbatch.program=curam.core.sl.infrastructure.assessment.intf.CREOLEBulkCaseChunkReassessmentStream.process\n```\n\n**Note:** The `args` key should become a sibling of the `command` key.\n\n</Column>\n</Row>\n</Tab>\n\n</Tabs>\n\nYou should now have YAML files for a chunker and streamer for bulk reassessment of food assistance case types.\n\n<InlineNotification>\n\nNewer versions of `kubectl` add a `metadata.ownerReferences` field to the output of `kubectl create job --from=...`.\nThis field must be removed before creating the new jobs from these YAML files.\n\n</InlineNotification>\n\n### Running batch streaming yaml files\n\nTo orchestrate the batch process, run the following command and repeat for the chunker and streamer.\n\n```shell\nkubectl create -f stream_foodassistance.yaml -n $namespace\nkubectl create -f chunker_foodassistance.yaml -n $namespace\n```\n\n### Post batch processing\n\nThe batch pods that are created for batch streaming are on demand.\n\nWhen the batch processes finish, the pods remain in a `completed` state.\nWhile this does not consume CPU or Memory resources on the worker nodes of your cluster, this may place added pressure on the Kubernetes API server\n\n<InlineNotification>\n\nA [TTL Controller for Finished Resources](https://kubernetes.io/docs/concepts/workloads/controllers/ttlafterfinished/) is available in Kubernetes, which will automatically clean up any completed pods after a specified length of time.\nHowever, this feature is in Alpha state and has to be enabled using a feature flag on both the `kube-apiserver` and `kube-controller-manager`.\n\n**Note:** It is not recommended to use Alpha features in a production environment.\n\n</InlineNotification>\n","fileAbsolutePath":"/home/runner/work/spm-kubernetes/spm-kubernetes/src/pages/supporting-infrastructure/batch/batch-streaming.mdx"}}},"staticQueryHashes":["1364590287","137577622","2102389209","2456312558","2746626797","3018647132","3037994772","56986546","768070550"]}